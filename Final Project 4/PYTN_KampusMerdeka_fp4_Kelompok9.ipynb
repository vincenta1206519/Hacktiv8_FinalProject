{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='text-align: center;'> Final Project 4 : Clustering with K-Means Clustering </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created by: Kelompok 9\n",
    "### Class: PYTN-KS18\n",
    "\n",
    "- ðŸ‘¤ **Member 1:** Vincent Tanaka - PYTN-KS18-013\n",
    "- ðŸ‘¤ **Member 2:** Audris Vondrea Wirduno - PYTN-KS18-02\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "*Project* ini akan membahas mengenai penggunaan *clustering* yang merupakan teknik *unsupervised machine learning* pada *dataset* pengguna kartu kredit yang terdiri dari ***9000*** pengguna. Hasil akhir yang diharapkan adalah untuk mendapatkan strategi marketing yang efektif dari hasil *clustering* yang dilakukan.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.1 Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Pada *notebook* ini studi kasus yang dibahas berupa prilaku **9000** pengguna kartu kredit aktif dalam waktu 6 Bulan. Dengan jumlah variabel sebanyak **18**. Hasil yang diharapkan dari studi kasus ini dalah untuk membuat sebuah strategi marketing dari *clustering* yang akan dilakukan setelahnya.  \n",
    "Data didapatkan dari kaggle dengan link : https://www.kaggle.com/datasets/arjunbhasin2013/ccdata\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.2 About Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah deskripsi **18** variabel dari *dataset* yang akan digunakan:  \n",
    "1. **CUSTID** - Identification of Credit Card holder (Categorical)\n",
    "2. **BALANCE** - Balance amount left in their account to make purchases\n",
    "3. **BALANCEFREQUENCY** - How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n",
    "4. **PURCHASES** - Amount of purchases made from account\n",
    "5. **ONEOFFPURCHASES** - Maximum purchase amount done in one-go\n",
    "6. **INSTALLMENTSPURCHASES** - Amount of purchase done in installment\n",
    "7. **CASHADVANCE** - Cash in advance given by the user\n",
    "8. **PURCHASESFREQUENCY** - How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n",
    "9. **ONEOFFPURCHASESFREQUENCY** - How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n",
    "10. **PURCHASESINSTALLMENTSFREQUENCY** - How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n",
    "11. **CASHADVANCEFREQUENCY** - How frequently the cash in advance being paid\n",
    "12. **CASHADVANCETRX** - Number of Transactions made with \"Cash in Advance\"\n",
    "13. **PURCHASESTRX** - Number of purchase transactions made\n",
    "14. **CREDITLIMIT** - Limit of Credit Card for user\n",
    "15. **PAYMENTS** - Amount of Payment done by user\n",
    "16. **MINIMUM_PAYMENTS** - Minimum amount of payments made by user\n",
    "17. **PRCFULLPAYMENT** - Percent of full payment paid by user\n",
    "18. **TENURE** - Tenure of credit card service for user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.3 Project Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapun *Objective* dari pengerjaan *final project* ini yaitu :  \n",
    "1. Melakukan eksplorasi data mengenai hubungan antar variabel dari data.\n",
    "2. Melakukan *preprocessing data* sebelum pemodelan data.\n",
    "3. Melakukan pemodelan data *clustering* yang sudah ditentukan\n",
    "4. Mengambil kesimpulan dari apa yang sudah dilakukan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the needed libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from kneed import KneeLocator\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. *Dataset Loading*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset given, name it as df\n",
    "df = pd.read_csv('cc-dataset.zip',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 row of df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Disini bisa disimpulkan bahwa *dataset* yang akan digunakan memiliki data sebanyak **8950** baris dengan jumlah variabel sebanyak **18** kolom.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.1 Duplicate Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's a duplicated data\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil pengecekan tidak ada data duplikat pada *dataset* ini, proses dilanjut dengan meng-*handle missing values*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.2 Missing Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat **2** kolom yang memiliki *missing values*, yaitu **CREDIT_LIMIT(1 missing value)** dan **MINIMUM_PAYMENTS(313 missing values)**. Maka dari itu data tersebut akan dihandle menggunakan inputasi. Pertama-tama bisa dicek terlebih dahulu distribusi data pada kedua kolom tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns with missing values to plot\n",
    "columns_to_plot = ['CREDIT_LIMIT', 'MINIMUM_PAYMENTS']\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a figure and set its size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loop through the missing value columns list and create a subplot for each\n",
    "for i, column in enumerate(columns_to_plot, 1):\n",
    "    plt.subplot(1, 2, i)  # 1 row, 2 columns, subplot index\n",
    "    sns.histplot(df[column].dropna(), bins=30, kde=False, edgecolor='black')\n",
    "    plt.title(f'Data Distribution of {column}')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil ini bisa disimpulkan bahwa distribusi data kedua kolom yang memiliki *missing values* cenderung ke *right-skewed* dimana nilai *mean* < nilai *median*. Dengan kondisi tersebut data yang kosong akan diisi dengan nilai median dikarenakan distribusi data tidak normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of columns that need to be filled with median number\n",
    "columns_to_fill = ['CREDIT_LIMIT', 'MINIMUM_PAYMENTS']\n",
    "\n",
    "for columns in columns_to_fill:\n",
    "    median_value = df[columns].median()\n",
    "    df[columns].fillna(median_value, inplace=True)\n",
    "\n",
    "# Verify if NaN values have been filled\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disini data sudah tidak memiliki nilai kosong, proses bisa dilanjut dengan membuang kolom yang tidak diperlukan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.3 Dropping un-needed Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil tampilan *dataframe*, bisa disimpulkan bahwa kolom **CUST_ID** merupakan sebuah *primary key* dari *dataset*. Dikarenakan nilai didalamnya merupakan *unique value* yang berjumlah sebanyak **8950** *unique values*, maka kelompok 9 akan menetapkan bahwa kolom ini akan dibuang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='CUST_ID', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang, data uang akan digunakan hanya terdiri dari **17** kolom saja. Proses bisa dilanjutkan dengan melakukan *EDA*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.1 Measure of Central Tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Central Tendency of each numerical columns\n",
    "central_tendency = df.describe().T\n",
    "central_tendency.reset_index().rename(\n",
    "    columns={'index': 'Attributes'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DataFrame* tersebut menghitung beberapa *measure of central tendency* untuk setiap kolom yang memiliki tipe data numerik. *Measure of central tendency* yang dihitung melibatkan statistik deskriptif dasar, termasuk *mean* (rata-rata), *std* (standar deviasi), *min* (nilai minimum), 25% (kuartil pertama), 50% (median atau kuartil kedua), 75% (kuartil ketiga), dan *max* (nilai maksimum). *DataFrame* ini memberikan gambaran singkat tentang sebaran nilai di setiap kolom dan membantu dalam memahami *central tendency* dari dataset tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.2 Measure of Central Tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that only contains numeric columns\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_df = df[numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute some measure of variability\n",
    "range_values = numeric_df.max() - numeric_df.min()\n",
    "variance_values = numeric_df.var()\n",
    "std_deviation_values = numeric_df.std()\n",
    "iqr_values = numeric_df.quantile(0.75) - numeric_df.quantile(0.25)\n",
    "cv_values = std_deviation_values / numeric_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results into one dataframe\n",
    "variability_measures = pd.DataFrame({\n",
    "    'Range': range_values,\n",
    "    'Variance': variance_values,\n",
    "    'Std Deviation': std_deviation_values,\n",
    "    'IQR': iqr_values,\n",
    "    'CV': cv_values\n",
    "})\n",
    "variability_measures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam hasil diatas, telah dihitung beberapa ukuran variabilitas (*measure of variability*) untuk kolom-kolom numerik pada dataset pengguna kartu kredit dari *Kaggle*. Ini mencakup *Range* (Rentang), *Variance* (Varians), *Std Deviation* (Deviasi Standar), IQR (*Interquartile Range*), dan CV (*Coefficient of Variation*). Hasil ini memberikan gambaran tentang sebaran dan variasi data keuangan, seperti rentang batas kredit, variasi nilai transaksi, dan konsistensi tagihan. DataFrame yang dihasilkan menyajikan nilai-nilai tersebut untuk setiap kolom numerik dalam dataset, memberikan wawasan komprehensif terhadap karakteristik variabilitas data keuangan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.3 Simple Data Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of your DataFrame's columns\n",
    "columns = df.columns\n",
    "\n",
    "# Determine the number of rows/columns for the subplot grid\n",
    "n_rows = (len(columns) + 1) // 2\n",
    "n_cols = 2\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "plt.figure(figsize=(15, 5 * n_rows))\n",
    "\n",
    "for i, column in enumerate(columns):\n",
    "    plt.subplot(n_rows, n_cols, i + 1)\n",
    "    # Check if the data is numerical or categorical\n",
    "    if df[column].dtype in ['int64', 'float64']:\n",
    "        sns.histplot(df[column], kde=True, bins=30)\n",
    "    else:\n",
    "        sns.countplot(x=column, data=df)\n",
    "    plt.title(column)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil *plot* diatas merupakan distribusi data yang ada pada tiap kolom *dataset*. Ada beberapa hal yang bisa disimpulkan dari hasil ini yaitu:\n",
    "- Banyak kolom yang memiliki grafik *right-skewed* dimana data-data tersebut memiliki nilai mean < dari nilai mediannya.\n",
    "- Pada kolom **'BALANCE'** dan **'BALANCE_FREQUENCY'** bisa disimpulkan bahwa para pengguna kartu kredit rajin dalam membayar kredit mereka bisa dilihar dengan kolom **'BALANCE'** yang sering muncul adalah 0. Diikuti dengan kolom **'BALANCE_FREQUENCY'** yang memiliki mayoritas nilai 1.0 yang membuktikan berapa seringnya kartu kredit tersebut digunakan.\n",
    "- Pada kolom **'TENURE'** bisa disimpulkan bahwa pengguna kartu kredit, lebih memilih pembayaran selama 12 bulan dalam kreditnya meskipun biasanya pembayaran dengan bulan yang lebih lama itu jauh lebih mahal di suku bunganya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of CREDIT_LIMIT vs BALANCE with a hue based on TENURE\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='CREDIT_LIMIT', y='BALANCE', hue='TENURE', palette='viridis', s=100)\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.title('Scatter Plot of Credit Limit vs Balance by Tenure (Grouped)')\n",
    "plt.xlabel('Credit Limit')\n",
    "plt.ylabel('Balance')\n",
    "plt.legend(title='Tenure')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisa dilihat dari pesebaran data berdasarkan kolom **'BALANCE'** dan **'CREDIT_LIMIT'** pengguna lebih banyak menggunakan TENURE selama 12 bulan. Dibuktikan dari titik bewarna kuning pada plot ini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f.1 Correlation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Choose a diverging color palette\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "# Generate a heatmap\n",
    "sns.heatmap(corr, annot=True, cmap=cmap, linewidths=.5)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah hasil *plot* korelasi yang dilakukan pada *DataFrame* menggunakan *Heatmap* dimana warna korelasi yang mengarah ke angka (1.0) atau merah menunjukkan berapa positifnya korelasi antar kolom yang ada *DataFrame*. Begitu sebaliknya dengan yang mengarak ke angka (0.) atay biru yang menunjukkan bahwa kolom tersebut memiliki korelasi negatif atau tidak berhubungan satu sama lain. Disini ada 1 kolom yang memiliki korelasi yang sangat kuat dengan kolom lainnya dengan nilai korelasi sebanyak **0.92** yaitu **'ONEOFF_PURCHASES'**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f.2 *Standardization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='justify'>\n",
    "Proses selanjutnya adalah scaling dataset. Disini dataset akan diubah menggunakan transformasi linear untuk menghasilkan cluster yang lebih bagus kualitasnya dengan mengelola variabilitas dataset.Standarisasi dataset akan diharapkan untuk meningkatkan peforma clustering nantinya.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the entire column\n",
    "X = pd.DataFrame(StandardScaler().fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f.3 *Inferential Statistic Test with a Hopkins Test*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses selanjutnya adalah untuk melakukan statistik inferensial menggunakan *Test Hopkins* untuk menghitung *clustering tendency*/kecenderungan pengelompokan data sekarang.\n",
    "Berikut adalah penjelasan Hipotesis Hopkins Test yang akan dilakukan :\n",
    "- H0: Dataset mungkin diclusterkan (contains meaningful clusters).\n",
    "- H1: Dataset tidak mungkin diclusterkan (no meaningful clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hopkins_statistic(X):\n",
    "    n, d = X.shape\n",
    "    m = int(0.1 * n)  # considering 10% of the total data\n",
    "    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n",
    "\n",
    "    rand_X = np.random.uniform(X.min(), X.max(), (m, d))\n",
    "    \n",
    "    uj = np.array([nbrs.kneighbors([rand_X[i]], return_distance=True)[0][0][0] for i in range(m)])\n",
    "    wj = np.array([nbrs.kneighbors([X.iloc[random.choice(range(n))]], return_distance=True)[0][0][0] for i in range(m)])\n",
    "\n",
    "    H = np.sum(uj) / (np.sum(uj) + np.sum(wj))\n",
    "    return H\n",
    "\n",
    "# Assuming 'X' is your standardized DataFrame\n",
    "hopkins_score = hopkins_statistic(X)\n",
    "print(\"Hopkins statistic:\", hopkins_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil pengecekan statistik hopkins diatas, bisa disimpulkan bahwa H0 diterima dimana dataset ini memiliki cluster yang berguna. Proses bisa dilanjut dengan implementasi PCA untuk mereduksi dimensi data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f.4 *Dimensionality Reduction with PCA*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PCA (principal component analysis)* merupakan metode yang biasa digunakan di *unsupervised machine learning* untuk meringkas tabel data multivariat dalam skalar besar sehingga bisa dijadikan variabel yang lebih kecil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Apply PCA to the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g. *Define the Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='justify'>\n",
    "\n",
    "Disini metode *clustering* yang akan dilakukan adalah *K-Means Clustering*. *K-Means Clustering* adalah metode *unsupervised machine mearning* yang biasa digunakan untuk *clustering*. Biasanay prosedurnya berupa klasifikasi kumpulan data tertentu kedalam sejumlah kluster yang ditentukan dengan huruf 'K' yang telah ditetapkan sebelumnya.  \n",
    "Sebelum dilakukan clustering, ada tahap yang bisa dilakukan sebelumnya yaitu untuk menentukan nilai K yang paling optimal.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Range of K values to try\n",
    "K_range = range(1, 10)\n",
    "\n",
    "# Lists to store results\n",
    "inertia = []\n",
    "times = []\n",
    "\n",
    "# Compute KMeans and record inertia and time for each k\n",
    "for k in K_range:\n",
    "    start_time = time.time()\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Append inertia and time\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "# Determine the elbow point using the kneed library\n",
    "kneedle = KneeLocator(K_range, inertia, curve='convex', direction='decreasing')\n",
    "elbow_k = kneedle.elbow\n",
    "\n",
    "# Plotting the results\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plotting the inertia (distortion scores)\n",
    "ax1.plot(K_range, inertia, 'bo-', label='Distortion Score')\n",
    "ax1.set_xlabel('Number of Clusters, K')\n",
    "ax1.set_ylabel('Distortion Score', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "ax1.set_title('Distortion Score and Computation Time for Different K Values')\n",
    "\n",
    "# Adding the Elbow annotation\n",
    "ax1.axvline(x=elbow_k, color='black', linestyle='--', label=f'Elbow at K={elbow_k}')\n",
    "\n",
    "# Creating a twin y-axis to plot the computation time\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(K_range, times, 'rv--', label='Fit Time (seconds)')\n",
    "ax2.set_ylabel('Fit Time (seconds)', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "# Adding legends\n",
    "fig.legend(loc='upper right', bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contoh diatas merupakan implemetasi *elbow method* untuk menentukan nilai K terbaik untuk *dataframe* ini. Didapatkan nilai **K** paling optimal adalah **4** cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with the optimal value on K\n",
    "final_kmeans = KMeans(n_clusters=elbow_k, n_init=10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disini proses dilanjut dengan melatih model menggunakan variabel yang sudah ditetapkan sebelumnya (X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to your data\n",
    "final_kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. *Evaluate the Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg = silhouette_score(X, clusters)\n",
    "print(\"The average silhouette_score is :\", silhouette_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
